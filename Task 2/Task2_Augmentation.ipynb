{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import re\n","import csv\n","import random\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from collections import Counter\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","from textattack.transformations import WordSwapRandomCharacterDeletion\n","from textattack.transformations import WordSwapQWERTY\n","from textattack.transformations import CompositeTransformation\n","from textattack.transformations import WordSwapChangeLocation\n","from textattack.transformations import WordSwapChangeName\n","from textattack.transformations import WordSwapChangeNumber\n","from textattack.transformations import WordSwapContract\n","from textattack.transformations import WordSwapWordNet\n","from textattack.transformations import WordSwapRandomCharacterSubstitution\n","from textattack.constraints.pre_transformation import RepeatModification\n","from textattack.constraints.pre_transformation import StopwordModification\n","from textattack.augmentation import Augmenter"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocssing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('2/train.csv',encoding='latin')\n","df = df.iloc[1:]\n","df = shuffle(df)\n","\n","train_size = int(0.8 * len(df))\n","val_size = int(0.1 * len(df))\n","test_size = len(df) - train_size - val_size\n","\n","train_data = df[:train_size]\n","val_data = df[train_size:train_size + val_size]\n","test_data = df[train_size + val_size:]\n","\n","train_data.columns = ['text', 'label']\n","val_data.columns = ['text', 'label']\n","test_data.columns = ['text', 'label']\n","\n","train_data.to_csv('2/train2.csv', index=False ,encoding='latin')\n","val_data.to_csv('2/validate2.csv', index=False,encoding='latin')\n","test_data.to_csv('2/test2.csv', index=False,encoding='latin')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1296,"status":"ok","timestamp":1665528850773,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"SpfJEZ1uC_h4"},"outputs":[],"source":["tweet_file = \"2/train2.csv\"\n","lowercase = False\n","tweet_df = pd.read_csv(tweet_file, delimiter=',', index_col=False, encoding='latin')\n","d = {\"label\": tweet_df['label'], \"text\": tweet_df['text']}\n","df = pd.DataFrame(data=d)\n","df[\"text\"] = df[\"text\"].fillna(\"\")  # 用空字符串替换缺失值\n","\n","\n","for i in range(len(df)):\n","    if \"&amp;\" in df[\"text\"][i]:\n","        df[\"text\"][i] = df[\"text\"][i].replace(\"&amp;\", \"&\")\n","    if lowercase:\n","        df[\"text\"][i] = df[\"text\"][i].lower()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["0       i took 18 codeine, 3 xanax &amp; 2 diazepam. h...\n","1       @IckyNocops iâve had chronic pain in the sam...\n","2       @mscisnt A big problem is that many chronic pa...\n","3       @WrestleCringe I take Adderall and wear button...\n","4       @StructuredSucc Aderall, vyvanse and Ritalin p...\n","                              ...                        \n","2997    @RawBeautyKristi My doc prescribed me Escitalo...\n","2998    I ate all of Deedeea??s food that weekend. But...\n","2999    cw/sex (TMI)\\n.\\nits so wild how much a orgasm...\n","3000    Dona??t blame the doctor for a??not understand...\n","3001    @Rosemcat1 @ibdgirl76 Exactly. This is whats c...\n","Name: text, Length: 3002, dtype: object"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tweet_df['text']"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665528850776,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"DrgcpRbFdwYK","outputId":"989d140e-b349-4e58-91d2-fa2c81dea89c"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are  566 positive examples in this dataset.\n","There are  2099 neutral examples in this dataset.\n","There are  312 negative examples in this dataset.\n"]}],"source":["print(\"There are \", len(df[df[\"label\"]=='positive']) , \"positive examples in this dataset.\")\n","print(\"There are \", len(df[df[\"label\"]=='neutral']), \"neutral examples in this dataset.\")\n","print(\"There are \", len(df[df[\"label\"]=='negative']), \"negative examples in this dataset.\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df.loc[ df[\"label\"] == 'positive', \"label\"] = 2 # Positive classification\n","df.loc[ df[\"label\"] == 'neutral', \"label\"] = 1 # Neutral classification\n","df.loc[ df[\"label\"] == 'negative', \"label\"] = 0 # Negative classification"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["valid_labels = [0, 1, 2]\n","df = df.dropna()\n","df = df[df['label'].isin(valid_labels)]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["0       i took 18 codeine, 3 xanax & 2 diazepam. had a...\n","1       @IckyNocops iâve had chronic pain in the sam...\n","2       @mscisnt A big problem is that many chronic pa...\n","3       @WrestleCringe I take Adderall and wear button...\n","4       @StructuredSucc Aderall, vyvanse and Ritalin p...\n","                              ...                        \n","2997    @RawBeautyKristi My doc prescribed me Escitalo...\n","2998    I ate all of Deedeea??s food that weekend. But...\n","2999    cw/sex (TMI)\\n.\\nits so wild how much a orgasm...\n","3000    Dona??t blame the doctor for a??not understand...\n","3001    @Rosemcat1 @ibdgirl76 Exactly. This is whats c...\n","Name: text, Length: 2977, dtype: object"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["X = df.text\n","X"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["0       1\n","1       1\n","2       1\n","3       1\n","4       2\n","       ..\n","2997    2\n","2998    1\n","2999    1\n","3000    1\n","3001    1\n","Name: label, Length: 2977, dtype: object"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["y = df.label\n","y"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9HOH8HEsFPAs"},"source":["### Augmentation\n","\n","* Use following transformations:\n","  * Random character swap\n","  * Character swap by adjacent QWERTY keyboard characters\n","  * Perform contractions (For example: \"I am\"->\"I'm\")\n","  * Swap words by Word Net synonyms "]},{"cell_type":"code","execution_count":17,"metadata":{"id":"1x_FQGcI37Vg"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\yan\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["539 tweets augmented.\n","Could not augment  339 tweets.\n"]}],"source":["transformation = CompositeTransformation([WordSwapRandomCharacterSubstitution(), WordSwapQWERTY(), WordSwapWordNet(), WordSwapContract()])\n","constraints = [RepeatModification(), StopwordModification()]\n","augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0.1, transformations_per_example=10)\n","print_var = 1\n","print_count = 0\n","\n","i = 0\n","neg = 0\n","for index, row in df.iterrows():\n","  if(row[\"label\"]==2):\n","    try:\n","      text = re.sub('@\\w+', '@', row[\"text\"])\n","      text = text.replace(\"'\", \"\")\n","      l = augmenter.augment(text)\n","      for n in range(len(l)):\n","        l[n] = l[n].replace(\"@\", \"@USER____\")\n","      new_serie = pd.Series(l)\n","      X = pd.concat([X, new_serie], ignore_index=True)\n","      ade_serie = pd.Series([2]*10)\n","      y = pd.concat([y, ade_serie], ignore_index=True)\n","      i = i + 1\n","    except IndexError:\n","      neg = neg + 1\n","      \n","  elif(row[\"label\"]==0):\n","    try:\n","      text = re.sub('@\\w+', '@', row[\"text\"])\n","      text = text.replace(\"'\", \"\")\n","      l = augmenter.augment(text)\n","      for n in range(len(l)):\n","        l[n] = l[n].replace(\"@\", \"@USER____\")\n","      new_serie = pd.Series(l)\n","      X = pd.concat([X, new_serie], ignore_index=True)\n","      ade_serie = pd.Series([0]*10)\n","      y = pd.concat([y, ade_serie], ignore_index=True)\n","      i = i + 1\n","    except IndexError:\n","      neg = neg + 1\n","        \n","print(i, \"tweets augmented.\")\n","print(\"Could not augment \", neg, \"tweets.\")"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Xf_WX8k24UWi"},"outputs":[{"data":{"text/plain":["Counter({1: 2099, 2: 3866, 0: 2402})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["Counter(y)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"v2scjhkxMkfJ"},"outputs":[],"source":["combined_data = list(zip(X, y))\n","random.shuffle(combined_data)\n","shuffled_X, shuffled_y = zip(*combined_data)\n","\n","with open(\"2/aug1.tsv\", 'wt', encoding='latin', newline='') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(X)):\n","      tsv_writer.writerow([X[i], y[i]])"]},{"cell_type":"markdown","metadata":{},"source":["### Resampling to make each class same number of samples"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are  2099 neural examples in this dataset.\n","There are  2402 negative examples in this dataset.\n","There are  3866 positive examples in this dataset.\n"]}],"source":["import pandas as pd\n","import csv\n","augument3 = \"2/aug1.tsv\"\n","df3 = pd.read_csv(augument3, sep='\\t', quoting=csv.QUOTE_NONE, encoding='latin')\n","print(\"There are \", len(df3[df3[\"label\"]==1]) , \"neural examples in this dataset.\")\n","print(\"There are \", len(df3[df3[\"label\"]==0]), \"negative examples in this dataset.\")\n","print(\"There are \", len(df3[df3[\"label\"]==2]), \"positive examples in this dataset.\")"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["df3 = df3.dropna(subset=['text', 'label'])"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\yan\\AppData\\Local\\Temp\\ipykernel_26860\\4167963144.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df3['label'] = df3['label'].astype(int)\n"]}],"source":["df3['label'] = df3['label'].astype(int)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["X3 = df3.text\n","y3 = df3.label"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# check none value and delete them\n","nan_mask = pd.isna(X3) | pd.isna(y3)\n","X_no_nan = X3[~nan_mask]\n","y_no_nan = y3[~nan_mask]"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["2    3866\n","0    2402\n","1    2099\n","Name: label, dtype: int64"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["y3.value_counts()"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6000\n"]},{"data":{"text/plain":["Counter({0: 2000, 1: 2000, 2: 2000})"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["# Random Oversampler\n","sampling_strategy = {\n","    0: 2000,  \n","    1: 2000, \n","    2: 2000  \n","}\n","oversampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n","X_oversampled, y_oversampled = oversampler.fit_resample(X_no_nan.to_numpy().reshape(-1,1), y_no_nan)\n","\n","print(len(X_oversampled))\n","Counter(y_oversampled)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["combined_data = list(zip(X_oversampled, y_oversampled))\n","random.shuffle(combined_data)\n","shuffled_X, shuffled_y = zip(*combined_data)\n","\n","with open(\"2/finalaugshuffle2.tsv\", 'wt', encoding='latin', newline='') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(shuffled_X)):\n","        tsv_writer.writerow([shuffled_X[i][0], shuffled_y[i]])"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["combined_data = list(zip(X_oversampled, y_oversampled))\n","random.shuffle(combined_data)\n","shuffled_X, shuffled_y = zip(*combined_data)\n","\n","with open(\"2/finalaugshuffle2utf.tsv\", 'wt', encoding='utf-8', newline='') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(shuffled_X)):\n","        tsv_writer.writerow([shuffled_X[i][0], shuffled_y[i]])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO6+GTLvZaa89JuxCGat1Ig","collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.11 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
